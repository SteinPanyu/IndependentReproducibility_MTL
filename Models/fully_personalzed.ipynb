{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import os\n",
    "\n",
    "\n",
    "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
    "\n",
    "PATH_DATA = 'data/D'\n",
    "PATH_ESM = os.path.join(PATH_DATA, 'EsmResponse.csv')\n",
    "PATH_PARTICIPANT = os.path.join(PATH_DATA, 'UserInfo.csv')\n",
    "PATH_SENSOR = os.path.join(PATH_DATA, 'Sensor')\n",
    "\n",
    "PATH_INTERMEDIATE = os.path.join('/home/uzair/Stress/StressDetection_Collaboration/Data_Processing_D1/data/intermediate')\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import cloudpickle\n",
    "import ray\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "def load(path: str):\n",
    "    with open(path, mode='rb') as f:\n",
    "        return cloudpickle.load(f)\n",
    "\n",
    "    \n",
    "def dump(obj, path: str):\n",
    "    with open(path, mode='wb') as f:\n",
    "        cloudpickle.dump(obj, f)\n",
    "        \n",
    "    \n",
    "def log(msg: any):\n",
    "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))\n",
    "\n",
    "\n",
    "def summary(x):\n",
    "    x = np.asarray(x)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        n = len(x)\n",
    "        # Here, uppercase np.dtype.kind corresponds to non-numeric data.\n",
    "        # Also, we view the boolean data as dichotomous categorical data.\n",
    "        if x.dtype.kind.isupper() or x.dtype.kind == 'b': \n",
    "            cnt = pd.Series(x).value_counts(dropna=False)\n",
    "            card = len(cnt)\n",
    "            cnt = cnt[:20]                \n",
    "            cnt_str = ', '.join([f'{u}:{c}' for u, c in zip(cnt.index, cnt)])\n",
    "            if card > 30:\n",
    "                cnt_str = f'{cnt_str}, ...'\n",
    "            return {\n",
    "                'n': n,\n",
    "                'cardinality': card,\n",
    "                'value_count': cnt_str\n",
    "            }\n",
    "        else: \n",
    "            x_nan = x[np.isnan(x)]\n",
    "            x_norm = x[~np.isnan(x)]\n",
    "            \n",
    "            tot = np.sum(x_norm)\n",
    "            m = np.mean(x_norm)\n",
    "            me = np.median(x_norm)\n",
    "            s = np.std(x_norm, ddof=1)\n",
    "            l, u = np.min(x_norm), np.max(x)\n",
    "            conf_l, conf_u = st.t.interval(0.95, len(x_norm) - 1, loc=m, scale=st.sem(x_norm))\n",
    "            n_nan = len(x_nan)\n",
    "            \n",
    "            return {\n",
    "                'n': n,\n",
    "                'sum': tot,\n",
    "                'mean': m,\n",
    "                'SD': s,\n",
    "                'med': me,\n",
    "                'range': (l, u),\n",
    "                'conf.': (conf_l, conf_u),\n",
    "                'nan_count': n_nan\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = os.path.join(PATH_INTERMEDIATE, 'stress-fixed.pkl')\n",
    "X, y, groups, t, datetimes = load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICIPANTS = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'PARTICIPANT_INFO.csv'),index_col = 'pcode')\n",
    "PINFO = PARTICIPANTS.assign(\n",
    "    BFI_OPN=lambda x: x['openness'],\n",
    "    BFI_CON=lambda x: x['conscientiousness'],\n",
    "    BFI_NEU=lambda x: x['neuroticism'],\n",
    "    BFI_EXT=lambda x: x['extraversion'],\n",
    "    BFI_AGR=lambda x: x['agreeableness'],\n",
    ")[[\n",
    "    'BFI_OPN', 'BFI_CON', 'BFI_NEU', 'BFI_EXT', 'BFI_AGR'\n",
    "]]\n",
    "PINFO = pd.get_dummies(PINFO, prefix_sep='=', dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PROC = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'LABELS_PROC.csv'), index_col=['pcode','timestamp'],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First timestamp: 2019-04-30 10:03:28+09:00\n",
      "Last timestamp: 2019-05-22 22:02:03+09:00\n"
     ]
    }
   ],
   "source": [
    "_df =LABELS_PROC\n",
    "_df.reset_index(level='timestamp', inplace=True)\n",
    "print('First timestamp:', _df['timestamp'].min())\n",
    "print('Last timestamp:', _df['timestamp'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_ranges = _df.groupby('pcode')['timestamp'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pid = set(LABELS_PROC.index.get_level_values('pcode').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINFO_valid = PINFO.loc[PINFO.index.isin(list_pid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the features into different categories\n",
    "feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]  \n",
    "feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]  \n",
    "feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]  \n",
    "feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]  \n",
    "feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]  \n",
    "feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]  \n",
    "feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
    "#Divide the time window features into sensor/past stress label\n",
    "feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]]  \n",
    "feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]]  \n",
    "feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  \n",
    "feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]]  \n",
    "feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]]  \n",
    "feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]]  \n",
    "feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]]\n",
    "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
    "feat_final = pd.concat([feat_baseline  ], axis=1)\n",
    "X = feat_final\n",
    "cats = X.columns[X.dtypes == bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['P01', 'P01', 'P01', ..., 'P80', 'P80', 'P80'], dtype='<U3')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, KFold, TimeSeriesSplit, LeavePGroupsOut, train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "normalize =True\n",
    "# Feature selection method\n",
    "selector = SelectFromModel(\n",
    "        estimator=LogisticRegression(penalty='l1', solver='liblinear', C=1, random_state=RANDOM_STATE, max_iter=4000),\n",
    "        threshold=0.005\n",
    "#         estimator=LinearSVC(\n",
    "#         penalty='l1',\n",
    "#         loss='squared_hinge',\n",
    "#         dual=False,\n",
    "#         tol=1e-3,\n",
    "#         C=1e-2,\n",
    "#         max_iter=5000,\n",
    "#         random_state=RANDOM_STATE\n",
    "#     )  \n",
    "    )\n",
    "select = [clone(selector)]\n",
    "oversample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_cat = np.asarray(sorted(cats))\n",
    "C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_users = [\"P05\", \"P09\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user: P01\n",
      "\n",
      " Number of 1s in y_user \n",
      "11\n",
      "\n",
      " Number of 1s in y_train \n",
      "9\n",
      "\n",
      " Number of 1s in y_test \n",
      "2\n",
      "\n",
      " Number of 1s in y_eval \n",
      "2\n",
      "[0]\ttrain-auc:0.95679\ttest-auc:0.50000\n",
      "[1]\ttrain-auc:0.99846\ttest-auc:0.30000\n",
      "[2]\ttrain-auc:0.99846\ttest-auc:0.40000\n",
      "[3]\ttrain-auc:0.99846\ttest-auc:0.30000\n",
      "[4]\ttrain-auc:0.99846\ttest-auc:0.40000\n",
      "[5]\ttrain-auc:0.99846\ttest-auc:0.30000\n",
      "[6]\ttrain-auc:0.99846\ttest-auc:0.40000\n",
      "[7]\ttrain-auc:0.99846\ttest-auc:0.40000\n",
      "[8]\ttrain-auc:0.99846\ttest-auc:0.40000\n",
      "[9]\ttrain-auc:0.99846\ttest-auc:0.40000\n",
      "Processing user: P02\n",
      "\n",
      " Number of 1s in y_user \n",
      "9\n",
      "\n",
      " Number of 1s in y_train \n",
      "7\n",
      "\n",
      " Number of 1s in y_test \n",
      "2\n",
      "\n",
      " Number of 1s in y_eval \n",
      "1\n",
      "[0]\ttrain-auc:0.99112\ttest-auc:0.28571\n",
      "[1]\ttrain-auc:0.99704\ttest-auc:0.28571\n",
      "[2]\ttrain-auc:0.99408\ttest-auc:0.28571\n",
      "[3]\ttrain-auc:0.99112\ttest-auc:0.28571\n",
      "[4]\ttrain-auc:0.99408\ttest-auc:0.28571\n",
      "[5]\ttrain-auc:0.99112\ttest-auc:0.28571\n",
      "[6]\ttrain-auc:0.99408\ttest-auc:0.28571\n",
      "[7]\ttrain-auc:0.99112\ttest-auc:0.28571\n",
      "[8]\ttrain-auc:1.00000\ttest-auc:0.14286\n",
      "[9]\ttrain-auc:1.00000\ttest-auc:0.14286\n",
      "Processing user: P03\n",
      "\n",
      " Number of 1s in y_user \n",
      "16\n",
      "\n",
      " Number of 1s in y_train \n",
      "13\n",
      "\n",
      " Number of 1s in y_test \n",
      "3\n",
      "\n",
      " Number of 1s in y_eval \n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzair/miniconda3/envs/sci-data/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n",
      "/home/uzair/miniconda3/envs/sci-data/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.99537\ttest-auc:0.83333\n",
      "[1]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "[2]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "[3]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "[4]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "[5]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "[6]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "[7]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "[8]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "[9]\ttrain-auc:1.00000\ttest-auc:0.83333\n",
      "Processing user: P06\n",
      "\n",
      " Number of 1s in y_user \n",
      "14\n",
      "\n",
      " Number of 1s in y_train \n",
      "11\n",
      "\n",
      " Number of 1s in y_test \n",
      "3\n",
      "\n",
      " Number of 1s in y_eval \n",
      "2\n",
      "[0]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[1]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[2]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[3]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[4]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[5]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[6]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[7]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[8]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "[9]\ttrain-auc:0.94215\ttest-auc:0.70833\n",
      "Processing user: P08\n",
      "\n",
      " Number of 1s in y_user \n",
      "48\n",
      "\n",
      " Number of 1s in y_train \n",
      "38\n",
      "\n",
      " Number of 1s in y_test \n",
      "10\n",
      "\n",
      " Number of 1s in y_eval \n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzair/miniconda3/envs/sci-data/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n",
      "/home/uzair/miniconda3/envs/sci-data/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.95333\ttest-auc:0.37500\n",
      "[1]\ttrain-auc:0.98500\ttest-auc:0.39583\n",
      "[2]\ttrain-auc:0.98722\ttest-auc:0.35417\n",
      "[3]\ttrain-auc:0.99000\ttest-auc:0.39583\n",
      "[4]\ttrain-auc:0.98778\ttest-auc:0.39583\n",
      "[5]\ttrain-auc:0.99000\ttest-auc:0.39583\n",
      "[6]\ttrain-auc:0.98778\ttest-auc:0.39583\n",
      "[7]\ttrain-auc:0.98778\ttest-auc:0.39583\n",
      "[8]\ttrain-auc:0.98778\ttest-auc:0.39583\n",
      "[9]\ttrain-auc:0.98778\ttest-auc:0.39583\n",
      "Processing user: P10\n",
      "\n",
      " Number of 1s in y_user \n",
      "6\n",
      "\n",
      " Number of 1s in y_train \n",
      "5\n",
      "\n",
      " Number of 1s in y_test \n",
      "1\n",
      "\n",
      " Number of 1s in y_eval \n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uzair/miniconda3/envs/sci-data/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 4, n_neighbors = 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m             sampler \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE, k_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Only oversample training data\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Train the XGBoost model\u001b[39;00m\n\u001b[1;32m     88\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_train, label\u001b[38;5;241m=\u001b[39my_train)\n",
      "File \u001b[0;32m~/miniconda3/envs/sci-data/lib/python3.9/site-packages/imblearn/base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sci-data/lib/python3.9/site-packages/imblearn/base.py:88\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m     86\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     94\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/miniconda3/envs/sci-data/lib/python3.9/site-packages/imblearn/over_sampling/_smote/base.py:633\u001b[0m, in \u001b[0;36mSMOTENC._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    630\u001b[0m X_ohe\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(X_ohe\u001b[38;5;241m.\u001b[39mdata, dtype\u001b[38;5;241m=\u001b[39mX_ohe\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedian_std_ \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    631\u001b[0m X_encoded \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mhstack((X_continuous, X_ohe), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 633\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# reverse the encoding of the categorical features\u001b[39;00m\n\u001b[1;32m    636\u001b[0m X_res_cat \u001b[38;5;241m=\u001b[39m X_resampled[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_features_\u001b[38;5;241m.\u001b[39msize :]\n",
      "File \u001b[0;32m~/miniconda3/envs/sci-data/lib/python3.9/site-packages/imblearn/over_sampling/_smote/base.py:355\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    352\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[0;32m--> 355\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_k_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    356\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[1;32m    357\u001b[0m     X_class, y\u001b[38;5;241m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    358\u001b[0m )\n\u001b[1;32m    359\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n",
      "File \u001b[0;32m~/miniconda3/envs/sci-data/lib/python3.9/site-packages/sklearn/neighbors/_base.py:810\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    808\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    811\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected n_neighbors <= n_samples, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but n_samples = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, n_neighbors = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_samples_fit, n_neighbors)\n\u001b[1;32m    813\u001b[0m     )\n\u001b[1;32m    815\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    816\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 4, n_neighbors = 6"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Lists to collect the metrics for each user\n",
    "all_accs = []\n",
    "all_aucs = []\n",
    "all_f1s = []\n",
    "all_f1p = []\n",
    "all_precisions = []\n",
    "all_recalls = []\n",
    "\n",
    "# Loop through each user group\n",
    "for user in PINFO_valid.index:\n",
    "    if user in skip_users:\n",
    "        continue\n",
    "    print(f\"Processing user: {user}\")\n",
    "    user_mask = groups == user\n",
    "    X_user, y_user = X[user_mask], np.array(y)[user_mask]\n",
    "    group_user = np.array(groups)[user_mask]\n",
    "    \n",
    "    # Count the number of 1s in y_user\n",
    "    count_ones = np.sum(y_user == 1)\n",
    "    print(\"\\n Number of 1s in y_user \")\n",
    "    print(count_ones)\n",
    "\n",
    "    \n",
    "    user_accs = []\n",
    "    user_aucs = []\n",
    "    user_f1s = []\n",
    "    user_f1p = []\n",
    "    user_precisions = []\n",
    "    user_recalls = []\n",
    "    \n",
    "    # Splitting the data into 80% training and 20% testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_user, y_user, test_size=0.2, random_state=RANDOM_STATE, stratify=y_user)\n",
    "    \n",
    "    count_ones = np.sum(y_train == 1)\n",
    "    print(\"\\n Number of 1s in y_train \")\n",
    "    print(count_ones)\n",
    "    \n",
    "    count_ones = np.sum(y_test == 1)\n",
    "    print(\"\\n Number of 1s in y_test \")\n",
    "    print(count_ones)\n",
    "    \n",
    "    # Split 20% of training set as eval set\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_STATE,  stratify=y_train)\n",
    "\n",
    "    count_ones = np.sum(y_eval == 1)\n",
    "    print(\"\\n Number of 1s in y_eval \")\n",
    "    print(count_ones)\n",
    "    \n",
    "    if normalize:\n",
    "        # Normalize numeric features\n",
    "        scaler = StandardScaler().fit(X_train[C_num])\n",
    "        X_train[C_num] = scaler.transform(X_train[C_num].copy())\n",
    "        X_eval[C_num] = scaler.transform(X_eval[C_num].copy())\n",
    "        X_test[C_num] = scaler.transform(X_test[C_num].copy())\n",
    "\n",
    "    if select:\n",
    "        if isinstance(select, SelectFromModel):\n",
    "            select = [select]\n",
    "        for i, s in enumerate(select):\n",
    "            # Fit feature selector only on training data\n",
    "            s.fit(X_train, y_train)\n",
    "            selected_features = X_train.columns[s.get_support()].tolist()\n",
    "\n",
    "            # Apply feature selection to train, eval, and test\n",
    "            X_train = X_train[selected_features].copy()\n",
    "            X_eval = X_eval[selected_features].copy()\n",
    "            X_test = X_test[selected_features].copy()\n",
    "\n",
    "    if oversample:\n",
    "        # Determine categorical features for SMOTENC\n",
    "        if len(C_cat):\n",
    "            M = np.isin(X_train.columns, C_cat)\n",
    "            sampler = SMOTENC(categorical_features=M, random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            minority_class_size = np.sum(y_train == 1)  # Assuming 1 is the minority class\n",
    "            if minority_class_size > 3:\n",
    "                sampler = SMOTE(random_state=RANDOM_STATE, k_neighbors=2)\n",
    "        # Only oversample training data\n",
    "        X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    deval = xgb.DMatrix(X_eval, label=y_eval)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "\n",
    "    # XGBoost parameters\n",
    "    param = {\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"objective\": 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'verbosity': 0,\n",
    "    }\n",
    "    evallist = [(dtrain, 'train'), (deval, 'test')]\n",
    "    bst = xgb.train(param, dtrain, early_stopping_rounds=10, evals=evallist)\n",
    "\n",
    "    y_real = dtest.get_label()\n",
    "    y_score = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "    # Predict binary outcomes instead of probabilities\n",
    "    y_pred = [1 if score >= 0.5 else 0 for score in y_score]\n",
    "\n",
    "    # Calculate metrics\n",
    "    user_accs.append(accuracy_score(y_real, y_pred))\n",
    "\n",
    "    if len(np.unique(y_real)) > 1:\n",
    "        user_aucs.append(roc_auc_score(y_true=y_real, y_score=y_score, average=None))\n",
    "    else:\n",
    "        user_aucs.append(np.nan)  # NaN indicates that ROC AUC couldn't be calculated\n",
    "\n",
    "    user_f1s.append(f1_score(y_true=y_real, y_pred=y_pred, pos_label=1, average='macro', zero_division=0))\n",
    "    user_precisions.append(precision_score(y_true=y_real, y_pred=y_pred, pos_label=1, average='macro', zero_division=0))\n",
    "    user_recalls.append(recall_score(y_true=y_real, y_pred=y_pred, pos_label=1, average='macro', zero_division=0))\n",
    "    user_f1p.append(f1_score(y_true=y_real, y_pred=y_pred, pos_label=1, average='binary', zero_division=0))\n",
    "\n",
    "        \n",
    "all_accs.extend(user_accs)\n",
    "all_aucs.extend(user_aucs)\n",
    "all_f1s.extend(user_f1s)\n",
    "all_f1p.extend(user_f1p)\n",
    "all_precisions.extend(user_precisions)\n",
    "all_recalls.extend(user_recalls)\n",
    "\n",
    "print(f\"User: {user}, Avg. Accuracy: {np.mean(user_accs):.4f}, Avg. AUC: {np.nanmean(user_aucs):.4f}, Avg. F1: {np.mean(user_f1s):.4f}\")\n",
    "\n",
    "# Overall metrics for all users\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"Average Accuracy: {np.mean(all_accs):.4f}\")\n",
    "print(f\"Average AUC: {np.nanmean(all_aucs):.4f}\")\n",
    "print(f\"Average Macro F1 Score: {np.mean(all_f1s):.4f}\")\n",
    "print(f\"Average Macro Precision: {np.mean(all_precisions):.4f}\")\n",
    "print(f\"Average Macro Recall: {np.mean(all_recalls):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"Average Accuracy: {np.mean(all_accs):.4f}\")\n",
    "print(f\"Average AUC: {np.nanmean(all_aucs):.4f}\")\n",
    "print(f\"Average Macro F1 Score: {np.mean(all_f1s):.4f}\")\n",
    "print(f\"Average Macro Precision: {np.mean(all_precisions):.4f}\")\n",
    "print(f\"Average Macro Recall: {np.mean(all_recalls):.4f}\")\n",
    "print(f\"Average Positive F1 Score: {np.mean(all_f1p):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
