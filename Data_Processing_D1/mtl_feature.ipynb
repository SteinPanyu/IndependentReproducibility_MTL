{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import os\n",
    "\n",
    "\n",
    "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
    "\n",
    "PATH_DATA = 'data/D'\n",
    "PATH_ESM = os.path.join(PATH_DATA, 'EsmResponse.csv')\n",
    "PATH_PARTICIPANT = os.path.join(PATH_DATA, 'UserInfo.csv')\n",
    "PATH_SENSOR = os.path.join(PATH_DATA, 'Sensor')\n",
    "\n",
    "PATH_INTERMEDIATE = os.path.join('data/intermediate')\n",
    "\n",
    "DATA_TYPES = {\n",
    "    'Acceleration': 'ACC',\n",
    "    'AmbientLight': 'AML',\n",
    "    'Calorie': 'CAL',\n",
    "    'Distance': 'DST',\n",
    "    'EDA': 'EDA',\n",
    "    'HR': 'HRT',\n",
    "    'RRI': 'RRI',\n",
    "    'SkinTemperature': 'SKT',\n",
    "    'StepCount': 'STP',\n",
    "    'UltraViolet': 'ULV',\n",
    "    'ActivityEvent': 'ACE',\n",
    "    'ActivityTransition': 'ACT',\n",
    "    'AppUsageEvent': 'APP',\n",
    "    'BatteryEvent': 'BAT',\n",
    "    'CallEvent': 'CAE',\n",
    "    'Connectivity': 'CON',\n",
    "    'DataTraffic': 'DAT',\n",
    "    'InstalledApp': 'INS',\n",
    "    'Location': 'LOC',\n",
    "    'MediaEvent': 'MED',\n",
    "    'MessageEvent': 'MSG',\n",
    "    'WiFi': 'WIF',\n",
    "    'ScreenEvent': 'SCR',\n",
    "    'RingerModeEvent': 'RNG',\n",
    "    'ChargeEvent': 'CHG',\n",
    "    'PowerSaveEvent': 'PWS',\n",
    "    'OnOffEvent': 'ONF'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import cloudpickle\n",
    "import ray\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "def load(path: str):\n",
    "    with open(path, mode='rb') as f:\n",
    "        return cloudpickle.load(f)\n",
    "\n",
    "    \n",
    "def dump(obj, path: str):\n",
    "    with open(path, mode='wb') as f:\n",
    "        cloudpickle.dump(obj, f)\n",
    "        \n",
    "    \n",
    "def log(msg: any):\n",
    "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))\n",
    "\n",
    "\n",
    "def summary(x):\n",
    "    x = np.asarray(x)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        n = len(x)\n",
    "        # Here, uppercase np.dtype.kind corresponds to non-numeric data.\n",
    "        # Also, we view the boolean data as dichotomous categorical data.\n",
    "        if x.dtype.kind.isupper() or x.dtype.kind == 'b': \n",
    "            cnt = pd.Series(x).value_counts(dropna=False)\n",
    "            card = len(cnt)\n",
    "            cnt = cnt[:20]                \n",
    "            cnt_str = ', '.join([f'{u}:{c}' for u, c in zip(cnt.index, cnt)])\n",
    "            if card > 30:\n",
    "                cnt_str = f'{cnt_str}, ...'\n",
    "            return {\n",
    "                'n': n,\n",
    "                'cardinality': card,\n",
    "                'value_count': cnt_str\n",
    "            }\n",
    "        else: \n",
    "            x_nan = x[np.isnan(x)]\n",
    "            x_norm = x[~np.isnan(x)]\n",
    "            \n",
    "            tot = np.sum(x_norm)\n",
    "            m = np.mean(x_norm)\n",
    "            me = np.median(x_norm)\n",
    "            s = np.std(x_norm, ddof=1)\n",
    "            l, u = np.min(x_norm), np.max(x)\n",
    "            conf_l, conf_u = st.t.interval(0.95, len(x_norm) - 1, loc=m, scale=st.sem(x_norm))\n",
    "            n_nan = len(x_nan)\n",
    "            \n",
    "            return {\n",
    "                'n': n,\n",
    "                'sum': tot,\n",
    "                'mean': m,\n",
    "                'SD': s,\n",
    "                'med': me,\n",
    "                'range': (l, u),\n",
    "                'conf.': (conf_l, conf_u),\n",
    "                'nan_count': n_nan\n",
    "            }\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def on_ray(*args, **kwargs):\n",
    "    try:\n",
    "        if ray.is_initialized():\n",
    "            ray.shutdown()\n",
    "        ray.init(*args, **kwargs)\n",
    "        yield None\n",
    "    finally:\n",
    "        ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Callable, Union, Tuple, List, Optional, Iterable\n",
    "from datetime import timedelta as td\n",
    "from scipy import stats\n",
    "import ray\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_na_check(_v):\n",
    "    _is_nan_inf = False\n",
    "    \n",
    "    try:\n",
    "        _is_nan_inf = np.isnan(_v) or np.isinf(_v)\n",
    "    except:\n",
    "        _is_nan_inf = False\n",
    "    \n",
    "    return _is_nan_inf or _v is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.integrate import simps\n",
    "\n",
    "def _extract_eda( d_val) :\n",
    "    # Set the sampling rate of your data (in Hz, e.g., 1000 samples per second)\n",
    "    sampling_rate = 2\n",
    "\n",
    "    _feature = {}\n",
    "    v =d_val\n",
    "    \n",
    "    if len(v) == 0:\n",
    "        return {}\n",
    "\n",
    "    # Raw EDA\n",
    "    mean_eda = np.mean(v)\n",
    "    max_eda = np.max(v)\n",
    "    min_eda = np.min(v)\n",
    "    std_eda = np.std(v)\n",
    "    peaks, _ = find_peaks(v)\n",
    "    num_peaks_eda = len(peaks)\n",
    "    auc_eda = simps(v, dx=1 / sampling_rate)\n",
    "    \n",
    "    if len(v) <10:\n",
    "        print(f\"Warning: EDA signal is too short: {len(v)} elements\")\n",
    "        # Combine features into a dictionary\n",
    "        all_features = {\n",
    "            'mean_eda': mean_eda,\n",
    "            'max_eda': max_eda,\n",
    "            'min_eda': min_eda,\n",
    "            'std_eda': std_eda,\n",
    "            'num_peaks_eda': num_peaks_eda,\n",
    "            'auc_eda': auc_eda\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # Decomposing EDA\n",
    "        data = nk.eda_phasic(v, sampling_rate=2)\n",
    "\n",
    "        # Tonic EDA\n",
    "        mean_tonic = np.mean(data['EDA_Tonic'])\n",
    "        max_tonic = np.max(data['EDA_Tonic'])\n",
    "        min_tonic = np.min(data['EDA_Tonic'])\n",
    "        std_tonic = np.std(data['EDA_Tonic'])\n",
    "        peaks, _ = find_peaks(data['EDA_Tonic'])\n",
    "        num_peaks_tonic = len(peaks)\n",
    "        auc_tonic = simps(data['EDA_Tonic'], dx=1 / sampling_rate)\n",
    "\n",
    "        # Phasic EDA\n",
    "        mean_phasic = np.mean(data['EDA_Phasic'])\n",
    "        max_phasic = np.max(data['EDA_Phasic'])\n",
    "        min_phasic = np.min(data['EDA_Phasic'])\n",
    "        std_phasic = np.std(data['EDA_Phasic'])\n",
    "        peaks, _ = find_peaks(data['EDA_Phasic'])\n",
    "        num_peaks_phasic = len(peaks)\n",
    "        auc_phasic = simps(data['EDA_Phasic'], dx=1 / sampling_rate)\n",
    "\n",
    "\n",
    "        # Combine features into a dictionary\n",
    "        all_features = {\n",
    "            'mean_eda': mean_eda,\n",
    "            'max_eda': max_eda,\n",
    "            'min_eda': min_eda,\n",
    "            'std_eda': std_eda,\n",
    "            'num_peaks_eda': num_peaks_eda,\n",
    "            'auc_eda': auc_eda,\n",
    "            'mean_tonic': mean_tonic,\n",
    "            'max_tonic': max_tonic,\n",
    "            'min_tonic': min_tonic,\n",
    "            'std_tonic': std_tonic,\n",
    "            'num_peaks_tonic': num_peaks_tonic,\n",
    "            'auc_tonic': auc_tonic,\n",
    "            'auc_phasic': auc_phasic,\n",
    "            'mean_phasic': mean_phasic,\n",
    "            'max_phasic': max_phasic,\n",
    "            'min_phasic': min_phasic,\n",
    "            'std_phasic': std_phasic,\n",
    "            'num_peaks_phasic': num_peaks_phasic,\n",
    "            'auc_phasic': auc_phasic\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    for feature, value in all_features.items():\n",
    "        _feature[f'EDA#{feature}']= value\n",
    "    \n",
    "    return _feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def _extract_rri( d_val) :\n",
    "    _feature = {}\n",
    "    v =d_val\n",
    "    \n",
    "    if len(v) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Extract features\n",
    "    mean = np.mean(v)\n",
    "    median = np.median(v)\n",
    "    maximum = np.max(v)\n",
    "    minimum = np.min(v)\n",
    "    std_dev = np.sqrt(np.var(v, ddof=1)) if len(v) > 1 else 0\n",
    "    kurt = stats.kurtosis(v, bias=False)\n",
    "    skw = stats.skew(v, bias=False)\n",
    "    # Calculate the slope of column\n",
    "    slope, _ = np.polyfit(np.arange(len(v)), v, 1)\n",
    "    percentile_80 = v.quantile(0.8)\n",
    "    percentile_20 = v.quantile(0.2)\n",
    "    \n",
    "    # RMSSD\n",
    "    rmssd = np.sqrt(np.mean(np.diff(v)**2))\n",
    "\n",
    "    # Combine features into a dictionary\n",
    "    all_features = {\n",
    "        \"mean\": mean,\n",
    "        \"median\": median,\n",
    "        \"maximum\": maximum,\n",
    "        \"minimum\": minimum,\n",
    "        \"std_dev\": std_dev,\n",
    "        \"kurt\": kurt,\n",
    "        \"skw\": skw,\n",
    "        \"slope\": slope,\n",
    "        \"percentile_80\": percentile_80,\n",
    "        \"percentile_20\": percentile_20,\n",
    "        \"rmssd\": rmssd\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    for feature, value in all_features.items():\n",
    "        _feature[f'HRV#{feature}']= value\n",
    "    \n",
    "    return _feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_numeric_feature(d_key, d_val) -> Dict:\n",
    "    feature = {}\n",
    "    v=d_val\n",
    "    hist, _ = np.histogram(v, bins='doane', density=False)\n",
    "    std = np.sqrt(np.var(v, ddof=1)) if len(v) > 1 else 0\n",
    "    v_norm = (v - np.mean(v)) / std if std != 0 else np.zeros(len(v))\n",
    "    feature[f'{d_key}#AVG'] = np.mean(v) # Sample mean\n",
    "    feature[f'{d_key}#STD'] = std # Sample standard deviation\n",
    "    if std !=0:\n",
    "        feature[f'{d_key}#SKW'] = stats.skew(v, bias=False) # Sample skewness\n",
    "        feature[f'{d_key}#KUR'] = stats.kurtosis(v, bias=False) # Sample kurtosis\n",
    "    else:\n",
    "        feature[f'{d_key}#SKW'] = -3 # Sample skewness\n",
    "        feature[f'{d_key}#KUR'] = -3 # Sample kurtosis\n",
    "    feature[f'{d_key}#ASC'] = np.sum(np.abs(np.diff(v))) # Abstract sum of changes\n",
    "    feature[f'{d_key}#BEP'] = stats.entropy(hist) # Binned entropy\n",
    "    feature[f'{d_key}#MED'] = np.median(v) # Median\n",
    "    feature[f'{d_key}#TSC'] = np.sqrt(np.sum(np.power(np.diff(v_norm), 2))) # Timeseries complexity\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_categorical_feature(cats, d_key, d_val) -> Dict:\n",
    "    feature = {}\n",
    "    v = d_val\n",
    "    cnt = v.value_counts()\n",
    "    val, sup = cnt.index, cnt.values\n",
    "    hist = {k: v for k, v in zip(val, sup)}\n",
    "\n",
    "    # Information Entropy\n",
    "    feature[f'{d_key}#ETP#'] = stats.entropy(sup / len(v))\n",
    "    # Abs. Sum of Changes\n",
    "    feature[f'{d_key}#ASC#'] = np.sum(v.values[1:] != v.values[:-1])\n",
    "    if len(cats) == 2: # Dichotomous categorical data\n",
    "        c = cats[0]\n",
    "        feature[f'{d_key}#RLV_SUP'] = hist[c] / len(v) if c in hist else 0\n",
    "    else:\n",
    "        for c in cats:\n",
    "            feature[f'{d_key}#RLV_SUP={c}'] = hist[c] / len(v)  if c in hist else 0\n",
    "            \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_timeWindow_feature(is_numeric, cats, d_key, d_val) -> Dict:\n",
    "    feature = {}\n",
    "    v = d_val\n",
    "    if d_key in ['CAE_DUR']:\n",
    "        feature = _extract_numeric_feature(d_key, v)\n",
    "        feature['CAE#FREQ'] = len(v)\n",
    "    elif d_key in ['LOC_CLS']:\n",
    "        feature = _extract_categorical_feature(cats, d_key, v)\n",
    "        feature['LOC#NumOfPlcVist'] = len(set(v))\n",
    "    elif d_key in ['EDA']:\n",
    "        feature = _extract_eda(v)\n",
    "    elif d_key in ['RRI']:\n",
    "        feature = _extract_rri(v)\n",
    "    else:\n",
    "        if is_numeric:\n",
    "            feature = _extract_numeric_feature(d_key, v)\n",
    "        else:\n",
    "            feature =_extract_categorical_feature(cats, d_key, v)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fucntion is based on the  towards circadian computing: \"early to bed and early to rise\"\n",
    "#makes some of us unhealthy and sleep derived\n",
    "theta=30\n",
    "def calculate_sleep_duration(s_on, s_off, theta):\n",
    "    # Merge s_on and s_off into a single DataFrame based on timestamp\n",
    "    df = pd.merge(pd.DataFrame({'timestamp': s_on, 'event': 'screen_on'}),\n",
    "                  pd.DataFrame({'timestamp': s_off, 'event': 'screen_off'}),\n",
    "                  how='outer', on='timestamp')\n",
    "    # fill missing values in event_x with values from event_y, and vice versa\n",
    "    df['event_x'] = df['event_x'].fillna(df['event_y'])\n",
    "    df['event_y'] = df['event_y'].fillna(df['event_x'])\n",
    "    # drop the event_x and event_y columns\n",
    "    df = df.drop(columns=['event_y']).rename(columns={'event_x': 'event'})\n",
    "    # Fill in missing timestamps with NaT and sort by timestamp\n",
    "    df = df.fillna(pd.NaT).sort_values('timestamp')\n",
    "    df=df.assign(\n",
    "         timestamp=lambda x: pd.to_datetime(x['timestamp'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ)\n",
    "     )\n",
    "    # Filter out screen-on events caused by notifications\n",
    "    mask = (df['event'] == 'screen_off') & ((df['timestamp'].diff().fillna(pd.NaT)  / pd.Timedelta(seconds=1)) > theta)\n",
    "    filtered_df = df[mask].reset_index(drop=True)\n",
    "    # Discard non-usage patterns that do not start between 9PM to 7AM (next day)\n",
    "    sleep_duration = pd.Series(dtype=float)\n",
    "    sleep_onset = pd.Series(dtype=\"datetime64[ns]\")\n",
    "    for i in range(len(filtered_df)-1):\n",
    "        if filtered_df.loc[i, 'timestamp'].hour >= 21 or filtered_df.loc[i, 'timestamp'].hour < 7:\n",
    "            non_usage_duration = filtered_df.loc[i+1, 'timestamp'] - filtered_df.loc[i, 'timestamp']\n",
    "            if non_usage_duration.total_seconds() > 0:\n",
    "                sleep_duration = pd.concat([sleep_duration, pd.Series(non_usage_duration.total_seconds())])\n",
    "                sleep_onset = pd.concat([sleep_onset , pd.Series(filtered_df.loc[i, 'timestamp'])])\n",
    "    # Calculate sleep midpoint and apply individual corrective term\n",
    "    if len(sleep_duration) > 0:\n",
    "        sleep_duration = sleep_duration.reset_index(drop=True)\n",
    "        sleep_onset  =sleep_onset.reset_index(drop=True)\n",
    "        sleep_midpoint = sleep_onset + pd.to_timedelta(sleep_duration/2, unit=\"s\")\n",
    "        return sleep_duration.max(), sleep_onset.iloc[sleep_duration.idxmax()], sleep_midpoint.iloc[sleep_duration.idxmax()]\n",
    "    else:\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_names = {\n",
    "    0: 'Dawn',\n",
    "    1: 'Morning',\n",
    "    2: 'Afternoon',\n",
    "    3: 'LateAfternoon',\n",
    "    4: 'Evening',\n",
    "    5: 'Night'\n",
    "}\n",
    "def _extract(\n",
    "        pid: str,\n",
    "        data: Dict[str, pd.Series],\n",
    "        label: pd.Series,\n",
    "        label_values: List[str],\n",
    "#        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "#        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "        categories: Dict[str, Optional[List[any]]] = None,\n",
    "        constant_features: Dict[str, any] = None,\n",
    "        resample_s: Dict[str, float] = None\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    _s = time.time()\n",
    "    log(f\"Begin feature extraction on {pid}'s data.\")\n",
    "    categories = categories or dict()\n",
    "    constant_features = constant_features or dict()\n",
    "    resample_s = resample_s or dict()\n",
    "    X, y, date_times = [], [], []\n",
    "#    count = 0\n",
    "    for timestamp in label.index:\n",
    "        row = dict()\n",
    "        #Find the start of today and yesterday for extracting today epoch features and yesterday epoch features\n",
    "        start_of_today = datetime(timestamp.year, timestamp.month, timestamp.day, tzinfo=timestamp.tzinfo)\n",
    "        start_of_today = pd.Timestamp(start_of_today.date(), tz=DEFAULT_TZ)\n",
    "        start_of_yesterday = timestamp - pd.Timedelta(days=1)\n",
    "        start_of_yesterday = pd.Timestamp(start_of_yesterday.date(), tz=DEFAULT_TZ)\n",
    "        label_cur = label.at[timestamp]\n",
    "        yesterday_time_windows = [\n",
    "                (start_of_yesterday + pd.Timedelta(hours=6), start_of_yesterday + pd.Timedelta(hours=9)),\n",
    "                (start_of_yesterday + pd.Timedelta(hours=9), start_of_yesterday + pd.Timedelta(hours=12)),\n",
    "                (start_of_yesterday + pd.Timedelta(hours=12), start_of_yesterday + pd.Timedelta(hours=15)),\n",
    "                (start_of_yesterday + pd.Timedelta(hours=15), start_of_yesterday + pd.Timedelta(hours=18)),\n",
    "                (start_of_yesterday + pd.Timedelta(hours=18), start_of_yesterday + pd.Timedelta(hours=21)),\n",
    "                (start_of_yesterday + pd.Timedelta(hours=21), start_of_yesterday + pd.Timedelta(hours=24))\n",
    "            ]\n",
    "        today_time_windows = []\n",
    "        for i in range(6):\n",
    "            start = start_of_today + pd.Timedelta(hours=i*3)\n",
    "            end = start_of_today + pd.Timedelta(hours=(i+1)*3)\n",
    "            if start <= timestamp:\n",
    "                today_time_windows.append((start, min(end, timestamp)))\n",
    "            else:\n",
    "                break\n",
    "        t = timestamp - td(milliseconds=1)\n",
    "        # Features relevant to participants' info\n",
    "        for d_key, d_val in constant_features.items():\n",
    "            row[d_key] = d_val\n",
    "        # Features from sensor data\n",
    "        for d_key, d_val in data.items():\n",
    "            is_numeric = d_key not in categories\n",
    "            cats = categories.get(d_key) or list()\n",
    "            d_val = d_val.sort_index()\n",
    "            # Features relevant to latest value of a given data\n",
    "            # These features are extracted only for bounded categorical data and numerical data.\n",
    "            if is_numeric or cats:\n",
    "                try:\n",
    "                    v = d_val.loc[:t].iloc[-1]\n",
    "                except (KeyError, IndexError):\n",
    "                    v = 0\n",
    "                if is_numeric:\n",
    "                    row[f'{d_key}#VAL'] = v\n",
    "                else:\n",
    "                    for c in cats:\n",
    "                        row[f'{d_key}#VAL={c}'] = v == c\n",
    "            # Features relevant to duration since the latest state change.\n",
    "            # These features are only for categorical data.\n",
    "            # In addition, duration since a given state is set recently is considered,\n",
    "            # that are available only at bounded categorical data.\n",
    "            if not is_numeric:\n",
    "                try:\n",
    "                    v = d_val.loc[:t]\n",
    "                    row[f'{d_key}#DSC'] = (t - v.index[-1]).total_seconds() if len(v) else -1.0\n",
    "                    for c in cats:\n",
    "                        v_sub = v.loc[lambda x: x == c].index\n",
    "                        row[f'{d_key}#DSC={c}'] = (t - v_sub[-1]).total_seconds() if len(v_sub) else -1.0\n",
    "                except (KeyError, IndexError):\n",
    "                    row[f'{d_key}#DSC'] = -1.0\n",
    "                    for c in cats:\n",
    "                        row[f'{d_key}#DSC={c}'] = -1.0\n",
    "            # Features extracted from time-windows\n",
    "            # These features requires resampling and imputation on each data.\n",
    "            sample_rate = RESAMPLE_S.get(d_key) or 1\n",
    "            d_val_res = d_val.resample(f'{sample_rate}S', origin='start')\n",
    "            if is_numeric:\n",
    "                try:\n",
    "                    # Your resampling code here...\n",
    "                    d_val_res = d_val_res.mean().interpolate(method='time').dropna()\n",
    "                except ValueError:\n",
    "                    # Save input data to a file or external storage for debugging...\n",
    "                    print(d_val_res)\n",
    "                    print(d_val)\n",
    "                    raise\n",
    "            else:\n",
    "                d_val_res = d_val_res.ffill().dropna()\n",
    "            #No resampling\n",
    "#             d_val_res =d_val\n",
    "           # Features extracted from immediate past time-windows\n",
    "            w_val = 15 * 60\n",
    "            try:\n",
    "                v = d_val_res.loc[t - td(seconds=w_val):t]\n",
    "            except (KeyError, IndexError):\n",
    "                continue\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                new_row = {f'{k}_ImmediatePast': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                row.update(new_row)\n",
    "            #Features extracted from yesterday epoch time windows\n",
    "            for count, (start, end) in enumerate(yesterday_time_windows):\n",
    "                # Get data for the current yesterday epoch time window\n",
    "                try:\n",
    "                    v = d_val_res.loc[start:end]\n",
    "                except (KeyError, IndexError):\n",
    "                    continue\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore')\n",
    "                    new_row = {f'{k}Yesterday{epoch_name}': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                    row.update(new_row)\n",
    "                    \n",
    "            #Features extracted from today epoch time windows until current time\n",
    "            for count, (start, end) in enumerate(today_time_windows):\n",
    "                # Get data for the current time window\n",
    "                try:\n",
    "                    v = d_val_res.loc[start:end]\n",
    "                except (KeyError, IndexError):\n",
    "                    continue\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter('ignore')\n",
    "                    new_row = {f'{k}Today{epoch_name}': v for k, v in _extract_timeWindow_feature(is_numeric, cats, d_key, v).items()}\n",
    "                    row.update(new_row)\n",
    "        #Sleep feature extracted from last night's data\n",
    "        onset_min = start_of_yesterday + pd.Timedelta(hours=21)\n",
    "        onset_max = start_of_today + pd.Timedelta(hours=14)\n",
    "        s_on =data['SCR_EVENT'].loc[data['SCR_EVENT']=='ON']\n",
    "        s_off =data['SCR_EVENT'].loc[data['SCR_EVENT']=='OFF']\n",
    "        duration, onset, midpoint =calculate_sleep_duration(s_on.loc[onset_min:onset_max].reset_index()['timestamp'], s_off.loc[onset_min:onset_max].reset_index()['timestamp'], theta)\n",
    "        if duration:\n",
    "            row['Sleep#Duration'] = duration\n",
    "            onset_hour = onset.hour\n",
    "            if onset_hour >=21:\n",
    "                row['Sleep#Onset'] = onset_hour - 21\n",
    "            else:\n",
    "                row['Sleep#Onset'] = onset_hour + 3\n",
    "        else:\n",
    "            row['Sleep#Duration'] = -1\n",
    "            row['Sleep#Onset'] = -1\n",
    "            \n",
    "        # Features relevant to time\n",
    "        day_of_week = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN'][t.isoweekday() - 1]\n",
    "        is_weekend = 'Y' if t.isoweekday() > 5 else 'N'\n",
    "        hour = t.hour\n",
    "        if 6 <= hour < 9:\n",
    "            hour_name = 'DAWN'\n",
    "        elif 9 <= hour < 12:\n",
    "            hour_name = 'MORNING'\n",
    "        elif 12 <= hour < 15:\n",
    "            hour_name = 'AFTERNOON'\n",
    "        elif 15 <= hour < 18:\n",
    "            hour_name = 'LATE_AFTERNOON'\n",
    "        elif 18 <= hour < 21:\n",
    "            hour_name = 'EVENING'\n",
    "        elif 21 <= hour < 24:\n",
    "            hour_name = 'NIGHT'\n",
    "        else:\n",
    "            hour_name = 'MIDNIGHT'\n",
    "        for d in ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN']:\n",
    "            row[f'Time#DOW={d}'] = d == day_of_week\n",
    "        for d in ['Y', 'N']:\n",
    "            row[f'Time#WKD={d}'] = d == is_weekend\n",
    "        for d in ['DAWN', 'MORNING', 'AFTERNOON', 'LATE_AFTERNOON', 'EVENING', 'NIGHT', 'MIDNIGHT']:\n",
    "            row[f'Time#HRN={d}'] = d == hour_name\n",
    "        try:\n",
    "            last_label = label.loc[label[:t].index.max()]\n",
    "        except (KeyError, IndexError):\n",
    "            last_label = -1\n",
    "        row[f'ESM#LastLabel'] = last_label\n",
    "        # Label values extracted from yesterday epochs\n",
    "        for count, (start, end) in enumerate(yesterday_time_windows):\n",
    "            try:\n",
    "                v = label.loc[start:end]\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                if len(label_values) <= 2: # Binary classification\n",
    "                    row[f'ESM#LIK#Yesterday{epoch_name}'] = np.sum(v == label_values[0]) / len(v) if len(v) > 0 else 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK={l}#Yesterday{epoch_name}'] = np.sum(v == l) / len(v) if len(v) > 0 else 0\n",
    "            except (KeyError, IndexError):\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                if len(label_values) <= 2:\n",
    "                    row[f'ESM#LIK#Yesterday{epoch_name}'] = 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK={l}#Yesterday{epoch_name}'] = 0\n",
    "        # Label values extracted from yesterday epochs\n",
    "        for count, (start, end) in enumerate(today_time_windows):\n",
    "            try:\n",
    "                v = label.loc[start:end]\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                if len(label_values) <= 2: # Binary classification\n",
    "                    row[f'ESM#LIK#Today{epoch_name}'] = np.sum(v == label_values[0]) / len(v) if len(v) > 0 else 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK={l}#Today{epoch_name}'] = np.sum(v == l) / len(v) if len(v) > 0 else 0\n",
    "            except (KeyError, IndexError):\n",
    "                epoch_name = epoch_names.get(count)\n",
    "                if len(label_values) <= 2:\n",
    "                    row[f'ESM#LIK#Today{epoch_name}'] = 0\n",
    "                else:\n",
    "                    for l in label_values:\n",
    "                        row[f'ESM#LIK={l}#Today{epoch_name}'] = 0           \n",
    "        # Label values extracted from immediate past\n",
    "        w_val = 15 * 60\n",
    "        try:\n",
    "            v = label.loc[t - td(seconds=w_val):t]\n",
    "            epoch_name = epoch_names.get(count)\n",
    "            if len(label_values) <= 2: # Binary classification\n",
    "                row[f'ESM#LIK#ImmediatePast'] = np.sum(v == label_values[0]) / len(v) if len(v) > 0 else 0\n",
    "            else:\n",
    "                for l in label_values:\n",
    "                    row[f'ESM#LIK={l}#ImmediatePast'] = np.sum(v == l) / len(v) if len(v) > 0 else 0\n",
    "        except (KeyError, IndexError):\n",
    "            epoch_name = epoch_names.get(count)\n",
    "            if len(label_values) <= 2:\n",
    "                row[f'ESM#LIK#ImmediatePast'] = 0\n",
    "            else:\n",
    "                for l in label_values:\n",
    "                    row[f'ESM#LIK={l}#ImmediatePast'] = 0  \n",
    "\n",
    "        row = {\n",
    "            k: 0.0 if _safe_na_check(v) else v\n",
    "            for k, v in row.items()\n",
    "        }\n",
    "\n",
    "        X.append(row)\n",
    "        y.append(label_cur)\n",
    "        date_times.append(timestamp)\n",
    "    \n",
    "    log(f\"Complete feature extraction on {pid}'s data ({time.time() - _s:.2f} s).\")\n",
    "    X = pd.DataFrame(X)\n",
    "    y = np.asarray(y)\n",
    "    group = np.repeat(pid, len(y))\n",
    "    date_times =  np.asarray(date_times)\n",
    "    return X, y, group, date_times\n",
    "def extract(\n",
    "        pids: Iterable[str],\n",
    "        data: Dict[str, pd.Series],\n",
    "        label: pd.Series,\n",
    "        label_values: List[str],\n",
    "#        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "#        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "        categories: Dict[str, Optional[List[any]]] = None,\n",
    "        constat_features: Dict[str, Dict[str, any]] = None,\n",
    "        resample_s: Dict[str, float] = None,\n",
    "        with_ray: bool=False\n",
    "):\n",
    "    if with_ray and not ray.is_initialized():\n",
    "        raise EnvironmentError('Ray should be initialized if \"with_ray\" is set as True.')\n",
    "    func = ray.remote(_extract).remote if with_ray else _extract\n",
    "    jobs = []\n",
    "    for pid in pids:\n",
    "        d = dict()\n",
    "        for k, v in data.items():\n",
    "            try:\n",
    "                d[k] = v.loc[(pid, )]\n",
    "                if k.startswith('LOC_'):\n",
    "                    d[k].index= pd.to_datetime( d[k].index, unit='ms', utc=True).tz_convert(DEFAULT_TZ)\n",
    "                d['SPEED'] = d.pop('LOC_SPEED')\n",
    "            except (KeyError, IndexError):\n",
    "                pass\n",
    "        job = func(\n",
    "            pid=pid, data=d, label=label.loc[(pid, )],\n",
    "            label_values=label_values,\n",
    "#            window_data=window_data,\n",
    "#            window_label=window_label,\n",
    "            categories=categories,\n",
    "            constant_features=constat_features[pid],\n",
    "            resample_s=resample_s\n",
    "        )\n",
    "        jobs.append(job)\n",
    "    jobs = ray.get(jobs) if with_ray else jobs\n",
    "    print([x.shape for _, x, _, _ in jobs])\n",
    "    X = pd.concat([x for x, _, _, _ in jobs], axis=0, ignore_index=True)\n",
    "    y = np.concatenate([x for _, x, _, _ in jobs], axis=0)\n",
    "    group = np.concatenate([x for _, _, x, _ in jobs], axis=0)\n",
    "    date_times = np.concatenate([x for _, _, _, x in jobs], axis=0)\n",
    "    t_s = date_times.min().normalize().timestamp()\n",
    "    t_norm = np.asarray(list(map(lambda x: x.timestamp() - t_s, date_times)))\n",
    "    C, DTYPE = X.columns, X.dtypes\n",
    "    X = X.fillna({\n",
    "        **{c: False for c in C[(DTYPE == object) | (DTYPE == bool)]},\n",
    "        **{c: 0.0 for c in C[(DTYPE != object) & (DTYPE != bool)]},\n",
    "    }).astype({\n",
    "        **{c: 'bool' for c in C[(DTYPE == object) | (DTYPE == bool)]},\n",
    "        **{c: 'float32' for c in C[(DTYPE != object) & (DTYPE != bool)]},\n",
    "    })\n",
    "    return X, y, group, t_norm, date_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cloudpickle\n",
    "\n",
    "LABEL_VALUES = [1, 0]\n",
    "RESAMPLE_S = {\n",
    "    'ACC_AXX': 0.25,\n",
    "    'ACC_AXY': 0.25,\n",
    "    'ACC_AXZ': 0.25,\n",
    "    'ACC_MAG': 0.25,\n",
    "    'EDA': 0.5,\n",
    "}\n",
    "\n",
    "\n",
    "CATEGORIES = {\n",
    "   'DST_MOT': ['IDLE', 'WALKING', 'JOGGING', 'RUNNING'],\n",
    "   'ULV_INT': ['NONE', 'LOW', 'MEDIUM', 'HIGH'],\n",
    "    'ACT': ['WALKING', 'STILL', 'IN_VEHICLE', 'ON_BICYCLE', 'RUNNING'],\n",
    "#    'APP_PAC': None,\n",
    "    'APP_CAT': ['SOCIAL','HEALTH','ENTER','WORK',\"INFO\"],\n",
    "   'BAT_STA': ['CHARGING', 'DISCHARGING', 'FULL', 'NOT_CHARGING'],\n",
    "#    'CAE': ['CALL', 'IDLE'],\n",
    "   'CON': ['DISCONNECTED', 'WIFI', 'MOBILE'],\n",
    "    'LOC_CLS': None,\n",
    "    'LOC_LABEL': ['eating','home','work','social','others'] ,\n",
    "    'SCR_EVENT':['ON', 'OFF', 'UNLOCK'],\n",
    "    'RNG': ['VIBRATE', 'SILENT', 'NORMAL'],\n",
    "    'CHG': ['DISCONNECTED', 'CONNECTED'],\n",
    "    'PWS': ['ACTIVATE', 'DEACTIVATE'],\n",
    "    'ONF': ['ON', 'OFF']\n",
    "}\n",
    "PARTICIPANTS = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'PARTICIPANT_INFO.csv'),index_col = 'pcode')\n",
    "PINFO = PARTICIPANTS.assign(\n",
    "    AGE=lambda x: x['age'],\n",
    "    GEN=lambda x: x['gender'],\n",
    "    BFI_OPN=lambda x: x['openness'],\n",
    "    BFI_CON=lambda x: x['conscientiousness'],\n",
    "    BFI_NEU=lambda x: x['neuroticism'],\n",
    "    BFI_EXT=lambda x: x['extraversion'],\n",
    "    BFI_AGR=lambda x: x['agreeableness'],\n",
    "    PSS=lambda x: x['PSS10'],\n",
    "    PHQ=lambda x: x['PHQ9'],\n",
    "    GHQ=lambda x: x['GHQ12'],\n",
    ")[[\n",
    "    'AGE', 'GEN', 'BFI_OPN', 'BFI_CON', 'BFI_NEU', 'BFI_EXT', 'BFI_AGR', 'PSS10', 'PHQ9', 'GHQ12'\n",
    "]]\n",
    "\n",
    "PINFO = pd.get_dummies(PINFO, prefix_sep='=', dtype=bool).to_dict('index')\n",
    "PINFO = {k: {f'PIF#{x}': y for x, y in v.items()} for k, v in PINFO.items()}\n",
    "DATA = load(os.path.join(PATH_INTERMEDIATE, 'proc.pkl'))\n",
    "LABELS_PROC = pd.read_csv(os.path.join(PATH_INTERMEDIATE, 'LABELS_PROC.csv'), index_col=['pcode','timestamp'],parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>responseTime</th>\n",
       "      <th>scheduledTime</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>attention</th>\n",
       "      <th>stress</th>\n",
       "      <th>duration</th>\n",
       "      <th>disturbance</th>\n",
       "      <th>change</th>\n",
       "      <th>valence_dyn</th>\n",
       "      <th>arousal_dyn</th>\n",
       "      <th>stress_dyn</th>\n",
       "      <th>disturbance_dyn</th>\n",
       "      <th>valence_fixed</th>\n",
       "      <th>arousal_fixed</th>\n",
       "      <th>stress_fixed</th>\n",
       "      <th>disturbance_fixed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcode</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">P01</th>\n",
       "      <th>2019-05-08 10:29:46+09:00</th>\n",
       "      <td>1557278986000</td>\n",
       "      <td>1.557279e+12</td>\n",
       "      <td>-3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08 11:16:12+09:00</th>\n",
       "      <td>1557281772000</td>\n",
       "      <td>1.557282e+12</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08 15:58:22+09:00</th>\n",
       "      <td>1557298702000</td>\n",
       "      <td>1.557299e+12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08 16:41:51+09:00</th>\n",
       "      <td>1557301311000</td>\n",
       "      <td>1.557301e+12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-08 17:27:42+09:00</th>\n",
       "      <td>1557304062000</td>\n",
       "      <td>1.557304e+12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  responseTime  scheduledTime  valence  \\\n",
       "pcode timestamp                                                          \n",
       "P01   2019-05-08 10:29:46+09:00  1557278986000   1.557279e+12       -3   \n",
       "      2019-05-08 11:16:12+09:00  1557281772000   1.557282e+12       -3   \n",
       "      2019-05-08 15:58:22+09:00  1557298702000   1.557299e+12        3   \n",
       "      2019-05-08 16:41:51+09:00  1557301311000   1.557301e+12        3   \n",
       "      2019-05-08 17:27:42+09:00  1557304062000   1.557304e+12        3   \n",
       "\n",
       "                                 arousal  attention  stress  duration  \\\n",
       "pcode timestamp                                                         \n",
       "P01   2019-05-08 10:29:46+09:00        3          3       3       5.0   \n",
       "      2019-05-08 11:16:12+09:00       -2          2       2      15.0   \n",
       "      2019-05-08 15:58:22+09:00        3          3      -3      20.0   \n",
       "      2019-05-08 16:41:51+09:00        3          3      -3      30.0   \n",
       "      2019-05-08 17:27:42+09:00        3          3      -3      20.0   \n",
       "\n",
       "                                 disturbance  change  valence_dyn  \\\n",
       "pcode timestamp                                                     \n",
       "P01   2019-05-08 10:29:46+09:00           -1      -3            0   \n",
       "      2019-05-08 11:16:12+09:00            3      -2            0   \n",
       "      2019-05-08 15:58:22+09:00            2       0            1   \n",
       "      2019-05-08 16:41:51+09:00            1       2            1   \n",
       "      2019-05-08 17:27:42+09:00            2       2            1   \n",
       "\n",
       "                                 arousal_dyn  stress_dyn  disturbance_dyn  \\\n",
       "pcode timestamp                                                             \n",
       "P01   2019-05-08 10:29:46+09:00            1           1                0   \n",
       "      2019-05-08 11:16:12+09:00            0           1                1   \n",
       "      2019-05-08 15:58:22+09:00            1           0                1   \n",
       "      2019-05-08 16:41:51+09:00            1           0                1   \n",
       "      2019-05-08 17:27:42+09:00            1           0                1   \n",
       "\n",
       "                                 valence_fixed  arousal_fixed  stress_fixed  \\\n",
       "pcode timestamp                                                               \n",
       "P01   2019-05-08 10:29:46+09:00              0              1             1   \n",
       "      2019-05-08 11:16:12+09:00              0              0             1   \n",
       "      2019-05-08 15:58:22+09:00              1              1             0   \n",
       "      2019-05-08 16:41:51+09:00              1              1             0   \n",
       "      2019-05-08 17:27:42+09:00              1              1             0   \n",
       "\n",
       "                                 disturbance_fixed  \n",
       "pcode timestamp                                     \n",
       "P01   2019-05-08 10:29:46+09:00                  0  \n",
       "      2019-05-08 11:16:12+09:00                  1  \n",
       "      2019-05-08 15:58:22+09:00                  1  \n",
       "      2019-05-08 16:41:51+09:00                  1  \n",
       "      2019-05-08 17:27:42+09:00                  1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_PROC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participationStartDate</th>\n",
       "      <th>participationStartTimestamp</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>openness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>PSS10</th>\n",
       "      <th>PHQ9</th>\n",
       "      <th>GHQ12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P01</th>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "      <td>1557241200000</td>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P02</th>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "      <td>1557241200000</td>\n",
       "      <td>21</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P03</th>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "      <td>1557241200000</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P04</th>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "      <td>1557241200000</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P05</th>\n",
       "      <td>2019-05-08 00:00:00+09:00</td>\n",
       "      <td>1557241200000</td>\n",
       "      <td>27</td>\n",
       "      <td>F</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          participationStartDate  participationStartTimestamp  age gender  \\\n",
       "pcode                                                                       \n",
       "P01    2019-05-08 00:00:00+09:00                1557241200000   27      M   \n",
       "P02    2019-05-08 00:00:00+09:00                1557241200000   21      M   \n",
       "P03    2019-05-08 00:00:00+09:00                1557241200000   24      F   \n",
       "P04    2019-05-08 00:00:00+09:00                1557241200000   23      M   \n",
       "P05    2019-05-08 00:00:00+09:00                1557241200000   27      F   \n",
       "\n",
       "       openness  conscientiousness  neuroticism  extraversion  agreeableness  \\\n",
       "pcode                                                                          \n",
       "P01          11                 11            3             4             13   \n",
       "P02          14                  5           12            14              5   \n",
       "P03          10                 15            8             7             11   \n",
       "P04          12                 11            8             6             11   \n",
       "P05          10                 11           13            10              6   \n",
       "\n",
       "       PSS10  PHQ9  GHQ12  \n",
       "pcode                      \n",
       "P01       13     0      1  \n",
       "P02       27     6     18  \n",
       "P03       18     2      6  \n",
       "P04       20     1      9  \n",
       "P05       25    14      9  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARTICIPANTS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 10:12:24,949\tWARNING services.py:1816 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=7.79gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2023-07-06 10:12:25,245\tINFO worker.py:1616 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_extract pid=343840)\u001b[0m [23-07-06 10:12:36] Begin feature extraction on P01's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343832)\u001b[0m [23-07-06 10:12:44] Begin feature extraction on P03's data.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_extract pid=343839)\u001b[0m [23-07-06 10:12:56] Begin feature extraction on P05's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343831)\u001b[0m [23-07-06 10:13:16] Begin feature extraction on P06's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343844)\u001b[0m [23-07-06 10:13:44] Begin feature extraction on P08's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343836)\u001b[0m [23-07-06 10:14:20] Begin feature extraction on P09's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343838)\u001b[0m [23-07-06 10:15:05] Begin feature extraction on P10's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343843)\u001b[0m [23-07-06 10:16:04] Begin feature extraction on P12's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343842)\u001b[0m [23-07-06 10:17:25] Begin feature extraction on P13's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343830)\u001b[0m [23-07-06 10:19:15] Begin feature extraction on P15's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343834)\u001b[0m [23-07-06 10:20:45] Begin feature extraction on P19's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343837)\u001b[0m [23-07-06 10:22:35] Begin feature extraction on P21's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343829)\u001b[0m [23-07-06 10:24:22] Begin feature extraction on P23's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343835)\u001b[0m [23-07-06 10:26:12] Begin feature extraction on P26's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m [23-07-06 10:28:17] Begin feature extraction on P28's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2624 MiB, 13 objects, write throughput 21 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 4251 MiB, 22 objects, write throughput 24 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343836)\u001b[0m Warning: EDA signal is too short: 4 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m Warning: EDA signal is too short: 1 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343840)\u001b[0m [23-07-06 12:56:51] Complete feature extraction on P01's data (9855.96 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343840)\u001b[0m [23-07-06 12:57:06] Begin feature extraction on P30's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343830)\u001b[0m [23-07-06 13:01:30] Complete feature extraction on P15's data (9735.29 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343832)\u001b[0m [23-07-06 13:01:39] Complete feature extraction on P03's data (10134.16 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343830)\u001b[0m [23-07-06 13:01:54] Begin feature extraction on P31's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343832)\u001b[0m [23-07-06 13:02:27] Begin feature extraction on P32's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343833)\u001b[0m [23-07-06 13:20:14] Complete feature extraction on P02's data (11255.87 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343833)\u001b[0m [23-07-06 13:20:35] Begin feature extraction on P33's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343838)\u001b[0m [23-07-06 13:21:23] Complete feature extraction on P10's data (11178.39 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343838)\u001b[0m [23-07-06 13:22:10] Begin feature extraction on P35's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343843)\u001b[0m [23-07-06 13:42:18] Complete feature extraction on P12's data (12373.99 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343843)\u001b[0m [23-07-06 13:42:49] Begin feature extraction on P39's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343831)\u001b[0m [23-07-06 13:46:30] Complete feature extraction on P06's data (12793.90 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343831)\u001b[0m [23-07-06 13:46:50] Begin feature extraction on P40's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343839)\u001b[0m [23-07-06 13:56:58] Complete feature extraction on P05's data (13442.17 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343839)\u001b[0m [23-07-06 13:57:45] Begin feature extraction on P42's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343837)\u001b[0m [23-07-06 14:02:08] Complete feature extraction on P21's data (13172.89 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343837)\u001b[0m [23-07-06 14:02:19] Begin feature extraction on P45's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343829)\u001b[0m [23-07-06 14:11:51] Complete feature extraction on P23's data (13649.01 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343829)\u001b[0m [23-07-06 14:12:18] Begin feature extraction on P47's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343842)\u001b[0m [23-07-06 14:15:13] Complete feature extraction on P13's data (14267.89 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343842)\u001b[0m [23-07-06 14:15:38] Begin feature extraction on P48's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343838)\u001b[0m Warning: EDA signal is too short: 4 elements\n",
      "\u001b[2m\u001b[36m(_extract pid=343834)\u001b[0m [23-07-06 14:22:47] Complete feature extraction on P19's data (14521.30 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343834)\u001b[0m [23-07-06 14:23:13] Begin feature extraction on P49's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343836)\u001b[0m [23-07-06 14:24:49] Complete feature extraction on P09's data (15028.40 s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 8295 MiB, 55 objects, write throughput 25 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_extract pid=343836)\u001b[0m [23-07-06 14:25:15] Begin feature extraction on P50's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343844)\u001b[0m [23-07-06 15:12:35] Complete feature extraction on P08's data (17931.19 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343844)\u001b[0m [23-07-06 15:13:03] Begin feature extraction on P51's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343833)\u001b[0m [23-07-06 16:05:58] Complete feature extraction on P33's data (9923.21 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343833)\u001b[0m [23-07-06 16:06:15] Begin feature extraction on P52's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343835)\u001b[0m [23-07-06 16:08:11] Complete feature extraction on P26's data (20519.01 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343835)\u001b[0m [23-07-06 16:08:31] Begin feature extraction on P53's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m [23-07-06 16:22:56] Complete feature extraction on P28's data (21279.30 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m [23-07-06 16:23:19] Begin feature extraction on P55's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343832)\u001b[0m [23-07-06 16:27:36] Complete feature extraction on P32's data (12309.30 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343832)\u001b[0m [23-07-06 16:27:59] Begin feature extraction on P57's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343837)\u001b[0m [23-07-06 16:37:05] Complete feature extraction on P45's data (9286.10 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343837)\u001b[0m [23-07-06 16:37:19] Begin feature extraction on P60's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343830)\u001b[0m [23-07-06 16:50:38] Complete feature extraction on P31's data (13724.00 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343830)\u001b[0m [23-07-06 16:50:56] Begin feature extraction on P61's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343842)\u001b[0m [23-07-06 16:52:24] Complete feature extraction on P48's data (9405.90 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343842)\u001b[0m [23-07-06 16:52:45] Begin feature extraction on P66's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343838)\u001b[0m [23-07-06 17:12:54] Complete feature extraction on P35's data (13843.90 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343838)\u001b[0m [23-07-06 17:13:13] Begin feature extraction on P67's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343836)\u001b[0m [23-07-06 17:32:11] Complete feature extraction on P50's data (11215.99 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343836)\u001b[0m [23-07-06 17:32:30] Begin feature extraction on P69's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343843)\u001b[0m [23-07-06 17:49:59] Complete feature extraction on P39's data (14830.50 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343843)\u001b[0m [23-07-06 17:50:24] Begin feature extraction on P70's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343831)\u001b[0m [23-07-06 17:53:20] Complete feature extraction on P40's data (14790.79 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343831)\u001b[0m [23-07-06 17:53:40] Begin feature extraction on P72's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343840)\u001b[0m [23-07-06 17:56:46] Complete feature extraction on P30's data (17979.90 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343840)\u001b[0m [23-07-06 17:57:10] Begin feature extraction on P75's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343839)\u001b[0m [23-07-06 18:43:45] Complete feature extraction on P42's data (17159.69 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343839)\u001b[0m [23-07-06 18:44:11] Begin feature extraction on P76's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343833)\u001b[0m [23-07-06 19:08:36] Complete feature extraction on P52's data (10940.51 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343833)\u001b[0m [23-07-06 19:08:54] Begin feature extraction on P77's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343829)\u001b[0m [23-07-06 19:30:48] Complete feature extraction on P47's data (19110.70 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343829)\u001b[0m [23-07-06 19:31:08] Begin feature extraction on P78's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343844)\u001b[0m [23-07-06 19:45:24] Complete feature extraction on P51's data (16341.29 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343844)\u001b[0m [23-07-06 19:45:43] Begin feature extraction on P79's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343830)\u001b[0m [23-07-06 19:54:29] Complete feature extraction on P61's data (11012.80 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343830)\u001b[0m [23-07-06 19:54:51] Begin feature extraction on P80's data.\n",
      "\u001b[2m\u001b[36m(_extract pid=343834)\u001b[0m [23-07-06 19:58:43] Complete feature extraction on P49's data (20129.70 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343837)\u001b[0m [23-07-06 20:08:14] Complete feature extraction on P60's data (12654.99 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343841)\u001b[0m [23-07-06 20:12:20] Complete feature extraction on P55's data (13741.41 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343843)\u001b[0m [23-07-06 20:13:36] Complete feature extraction on P70's data (8591.60 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343832)\u001b[0m [23-07-06 20:23:11] Complete feature extraction on P57's data (14112.29 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343842)\u001b[0m [23-07-06 20:23:20] Complete feature extraction on P66's data (12635.01 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343835)\u001b[0m [23-07-06 21:04:22] Complete feature extraction on P53's data (17750.89 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343838)\u001b[0m [23-07-06 21:04:27] Complete feature extraction on P67's data (13874.40 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343833)\u001b[0m [23-07-06 21:07:50] Complete feature extraction on P77's data (7135.71 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343844)\u001b[0m [23-07-06 21:10:21] Complete feature extraction on P79's data (5078.70 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343839)\u001b[0m [23-07-06 21:12:49] Complete feature extraction on P76's data (8917.70 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343831)\u001b[0m [23-07-06 21:14:31] Complete feature extraction on P72's data (12050.91 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343840)\u001b[0m [23-07-06 21:14:44] Complete feature extraction on P75's data (11853.80 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343830)\u001b[0m [23-07-06 21:15:33] Complete feature extraction on P80's data (4841.35 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343836)\u001b[0m [23-07-06 21:15:41] Complete feature extraction on P69's data (13390.82 s).\n",
      "\u001b[2m\u001b[36m(_extract pid=343829)\u001b[0m [23-07-06 21:15:47] Complete feature extraction on P78's data (6278.62 s).\n",
      "[(40,), (51,), (44,), (51,), (49,), (69,), (55,), (47,), (45,), (54,), (39,), (52,), (50,), (49,), (76,), (83,), (70,), (53,), (52,), (39,), (51,), (60,), (57,), (70,), (38,), (77,), (37,), (79,), (43,), (63,), (43,), (76,), (56,), (57,), (61,), (73,), (52,), (65,), (76,), (37,), (65,), (73,), (65,), (44,), (50,), (36,), (47,)]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pandas.errors import PerformanceWarning\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=PerformanceWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "\n",
    "with on_ray(num_cpus=16):\n",
    "    for l in ['stress']:\n",
    "        labels = LABELS_PROC[f'{l}_dyn']\n",
    "        pids = labels.index.get_level_values('pcode').unique()\n",
    "        feat = extract(\n",
    "            pids=pids,\n",
    "            data=DATA,\n",
    "            label=labels,\n",
    "            label_values=LABEL_VALUES,\n",
    "            categories=CATEGORIES,\n",
    "            constat_features=PINFO,\n",
    "            resample_s=RESAMPLE_S,\n",
    "            with_ray=True\n",
    "        )\n",
    "        dump(feat, os.path.join(PATH_INTERMEDIATE, f'{l}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
